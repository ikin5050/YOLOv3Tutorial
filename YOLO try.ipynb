{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c8bbb3",
   "metadata": {},
   "source": [
    "# Simple overview for using YOLOv3 model\n",
    "\n",
    "YOLOv3 is an object detection algorithm designed to operate in real time. The current version of yolov3 uses pytorch as a backend so you will need to have *torch* and *torchvision* installed, as well as *imageAI*. If you want to just get this model running feel free to skip but I will cover some of the background knowledge as to why and how YOLO works.\n",
    "\n",
    "YOLO is based on convolutional layers of a neural network. These layers convolve the input with kernels to learn features. For example:\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "This kernel will detect features that follow a line from the top left of the image to the bottom right. YOLO is special because it only uses 1x1 convolutions, meaning all the kernels are 1x1 sized. \n",
    "\n",
    "The exact method in which YOLO detects objects is as follows:\n",
    "The algorithm separates an image into a grid. Bounding boxes are determined by looking at the image with various windows and trying to associate pixels together. The bounding boxes get a score that represents how accurately each class describes what is visible in that window. Yolo is not recursive, it divides the image into a grid and each cell predicts a handful of boxes. This approach means that classification and detection are performed at the same time. Only the bounding boxes with highest confidence scores are displayed.\n",
    "\n",
    "## Retraining your YOLO model\n",
    "\n",
    "I'm not going to go super in depth on how to retrain your YOLO model, but know that you can do this to have it suit your own dataset more precisely. This is going to be very computationally intensive and you will need roughly 1000 images per class you want your class to recognize (and run data augmentation on this afterwards to make your model more robust). Data augmentation can include rotate, flip or zoom manipulations of your dataset. This will increase the transferability of your model. A link to an in-depth guide is here: https://imageai.readthedocs.io/en/latest/custom/index.html so follow that if you want to know more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4556a",
   "metadata": {},
   "source": [
    "## What is our example?\n",
    "\n",
    "The first cell will hold an example that takes your computer's inbuilt camera and classifies that datafeed as well as displays it. You will need the *.pt* pretrained model downloaded to your computer as we don't want to train a model from scratch, simply use it. *OpenCV* is also a requirement here. The *.h5* file that is also in this folder is for previous models of the YOLOv3 network that ran on a Keras/TensorFlow backend but provided for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dbf3862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection import ObjectDetection\n",
    "import cv2\n",
    "\n",
    "obj_detect = ObjectDetection()\n",
    "obj_detect.setModelTypeAsYOLOv3()\n",
    "#obj_detect.setModelPath(r\"C:\\Users\\ikin5\\Desktop\\PhD\\extra learning\\YOLO\\yolov3.pt\")\n",
    "obj_detect.setModelPath('path to .pt pretrained model')\n",
    "obj_detect.loadModel()\n",
    "\n",
    "\n",
    "cam_feed = cv2.VideoCapture(0) #set input pipeline to be camera feed\n",
    "cam_feed.set(cv2.CAP_PROP_FRAME_WIDTH, 650) #frame width and height, you can change these\n",
    "cam_feed.set(cv2.CAP_PROP_FRAME_HEIGHT, 750)\n",
    "\n",
    "while True:    #this will run infinitely if you let it\n",
    "    ret, img = cam_feed.read()   \n",
    "    annotated_image, preds = obj_detect.detectObjectsFromImage(input_image=img,\n",
    "                      output_type=\"array\",\n",
    "                      display_percentage_probability=False,\n",
    "                      display_object_name=True)\n",
    "    #Each frame is detected individually, with a good GPU this can be reasonable FPS \n",
    "    #but on a laptop with only CPU I only get around 8 fps, just keep this in mind\n",
    "\n",
    "    cv2.imshow(\"\", annotated_image)     \n",
    "    \n",
    "    if (cv2.waitKey(1) & 0xFF == ord(\"q\")) or (cv2.waitKey(1)==27):   #the escape key for this program is q\n",
    "        break\n",
    "\n",
    "cam_feed.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d9504",
   "metadata": {},
   "source": [
    "## Same but now from a stored file\n",
    "\n",
    "Just make sure the paths are correct, this adds bounding boxes and classifications to the file specified by input_file and stores the output in output_file. The file *input_video_yol.mp4* is provided to use as a test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection import VideoObjectDetection\n",
    "\n",
    "vid_obj_detect = VideoObjectDetection()\n",
    "\n",
    "vid_obj_detect.setModelTypeAsYOLOv3()\n",
    "\n",
    "vid_obj_detect.setModelPath(r\"C:/Datasets/yolo.h5\")\n",
    "vid_obj_detect.loadModel()\n",
    "\n",
    "input_file='path to input'\n",
    "output_file='path to output'\n",
    "\n",
    "detected_vid_obj = vid_obj_detect.detectObjectsFromVideo(\n",
    "    input_file_path=input_file,\n",
    "    output_file_path = output_file,\n",
    "    frames_per_second=15,\n",
    "    log_progress=True,\n",
    "    return_detected_frame = True,\n",
    ")\n",
    "\n",
    "print(detected_vid_obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
